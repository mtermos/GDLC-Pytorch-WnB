{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import warnings\n",
    "import wandb\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "\n",
    "from src.models import NIDSFNN, NIDSCNN, NIDSGRU, NIDSLSTM, CNN_LSTM\n",
    "from src.lightning_model import LitClassifier\n",
    "from src.lightning_data import LitDataModule\n",
    "from src.dataset.dataset_info import datasets, network_features\n",
    "from local_variables import local_datasets_path\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using_wandb = False\n",
    "save_top_k = 1\n",
    "\n",
    "multi_class = True\n",
    "use_centralities = False\n",
    "\n",
    "sort_timestamp = False\n",
    "sort_after_partition = False\n",
    "\n",
    "use_port_in_address = False\n",
    "generated_ips = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_datasets = [\n",
    "    # datasets[\"cic_ton_iot\"],\n",
    "    # datasets[\"cic_ids_2017\"],\n",
    "    # datasets[\"cic_ton_iot_modified\"],\n",
    "    datasets[\"ccd_inid_modified\"],\n",
    "    # datasets[\"nf_uq_nids_modified\"],\n",
    "    # datasets[\"edge_iiot\"],\n",
    "    # datasets[\"nf_cse_cic_ids2018\"],\n",
    "    # datasets[\"nf_uq_nids\"],\n",
    "    # datasets[\"x_iiot\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_after_partition_dataset = {\n",
    "    \"cic_ton_iot\": True,\n",
    "    \"cic_ids_2017\": True,\n",
    "    \"cic_ton_iot_modified\": True,\n",
    "    \"ccd_inid_modified\": False,\n",
    "    \"nf_uq_nids_modified\": False,\n",
    "    \"edge_iiot\": False,\n",
    "    \"nf_cse_cic_ids2018\": False,\n",
    "    \"nf_uq_nids\": False,\n",
    "    \"x_iiot\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_patience = max_epochs = 1\n",
    "# early_stopping_patience = 30\n",
    "batch_size = -1\n",
    "learning_rate = 0.0005\n",
    "\n",
    "weight_decay = 0\n",
    "fnn_hidden_units = [20]\n",
    "cnn_out_channels_list = [64]\n",
    "rnn_num_layers = 2\n",
    "rnn_hidden_size = 80\n",
    "dropout = 0.0\n",
    "sequence_length = 3\n",
    "stride = 1\n",
    "activation = F.relu\n",
    "\n",
    "run_dtime = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "exp_type = \"gdlc\"\n",
    "\n",
    "if multi_class:\n",
    "    exp_type += \"__multi_class\"\n",
    "\n",
    "if use_port_in_address:\n",
    "    exp_type += \"__ports\"\n",
    "\n",
    "if generated_ips:\n",
    "    exp_type += \"__generated_ips\"\n",
    "\n",
    "if sort_timestamp:\n",
    "    exp_type += \"__sorted\"\n",
    "elif sort_after_partition:\n",
    "    exp_type += \"__semisorted\"\n",
    "else:\n",
    "    exp_type += \"__unsorted\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_elapsed_dict = {}\n",
    "\n",
    "for dataset in my_datasets:\n",
    "\n",
    "    dataset_folder = os.path.join(local_datasets_path, dataset.name)\n",
    "    gdlc_folder = os.path.join(dataset_folder, exp_type)\n",
    "    logs_folder = os.path.join(\"logs\", dataset.name)\n",
    "    os.makedirs(logs_folder, exist_ok=True)\n",
    "\n",
    "    wandb_runs_path = os.path.join(\"logs\", \"wandb_runs\")\n",
    "    os.makedirs(wandb_runs_path, exist_ok=True)\n",
    "\n",
    "    labels_mapping = {0: \"Normal\", 1: \"Attack\"}\n",
    "    num_classes = 2\n",
    "    if multi_class:\n",
    "        with open(os.path.join(dataset_folder, \"labels_names.pkl\"), \"rb\") as f:\n",
    "            labels_names = pickle.load(f)\n",
    "        labels_mapping = labels_names[0]\n",
    "    num_classes = len(labels_mapping)\n",
    "\n",
    "    dataset_kwargs = dict(\n",
    "        sequence_length=sequence_length,\n",
    "        stride=stride,\n",
    "        using_masking=False,\n",
    "        masked_class=2,\n",
    "        num_workers=0,\n",
    "        device='cuda' if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "\n",
    "    data_module = LitDataModule(\n",
    "        gdlc_folder=gdlc_folder,\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        multi_class=multi_class,\n",
    "        use_centralities=use_centralities,\n",
    "        network_features=network_features[dataset.centralities_set-1],\n",
    "        **dataset_kwargs)\n",
    "    data_module.setup()\n",
    "\n",
    "    # print(f\"==>> data_module.batch_size: {data_module.batch_size}\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # criterion = nn.CrossEntropyLoss(weight=data_module.class_weights)\n",
    "\n",
    "    my_models = {\n",
    "        \"fnn\": NIDSFNN(hidden_units=fnn_hidden_units, num_features=data_module.num_features, num_classes=num_classes, dropout=dropout, use_bn=True),\n",
    "        \"cnn\": NIDSCNN(out_channels=cnn_out_channels_list, num_features=data_module.num_features, num_classes=num_classes, dropout=dropout),\n",
    "        \"gru\": NIDSGRU(num_features=data_module.num_features, hidden_size=rnn_hidden_size, num_layers=rnn_num_layers, num_classes=num_classes, dropout=dropout),\n",
    "        \"lstm\": NIDSLSTM(num_features=data_module.num_features, hidden_size=rnn_hidden_size, num_layers=rnn_num_layers, num_classes=num_classes, dropout=dropout),\n",
    "        # \"cnn_lstm\": CNN_LSTM(out_channels=cnn_out_channels_list, seq_length=64, num_classes=num_classes, lstm_hidden_size=rnn_hidden_size, lstm_num_layers=rnn_num_layers, lstm_dropout=dropout, final_dropout=dropout),\n",
    "    }\n",
    "\n",
    "    elapsed = {}\n",
    "\n",
    "    for model_name, model in my_models.items():\n",
    "        config = {\n",
    "            \"use_centralities\": use_centralities,\n",
    "            \"model_name\": model_name,\n",
    "            \"max_epochs\": max_epochs,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"fnn_hidden_units\": fnn_hidden_units,\n",
    "            \"cnn_out_channels_list\": cnn_out_channels_list,\n",
    "            \"rnn_num_layers\": rnn_num_layers,\n",
    "            \"rnn_hidden_size\": rnn_hidden_size,\n",
    "            \"activation\": activation.__name__,\n",
    "            \"dropout\": dropout,\n",
    "            \"multi_class\": multi_class,\n",
    "            \"early_stopping_patience\": early_stopping_patience,\n",
    "            \"sequence_length\": sequence_length,\n",
    "            \"stride\": stride,\n",
    "            \"run_dtime\": run_dtime,\n",
    "        }\n",
    "\n",
    "        graph_model = LitClassifier(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            learning_rate=learning_rate,\n",
    "            config=config,\n",
    "            model_name=model_name,\n",
    "            labels_mapping=labels_mapping,\n",
    "            weight_decay=weight_decay,\n",
    "            using_wandb=using_wandb,\n",
    "            multi_class=True,\n",
    "            label_col=dataset.label_col,\n",
    "            class_num_col=dataset.class_num_col,\n",
    "        )\n",
    "\n",
    "        data_module.set_model_type(model_type=model_name)\n",
    "\n",
    "        if using_wandb:\n",
    "            wandb_logger = WandbLogger(\n",
    "                project=f\"GNN-Analysis-{dataset.name}\",\n",
    "                name=model_name,\n",
    "                config=config,\n",
    "                save_dir=wandb_runs_path\n",
    "            )\n",
    "        else:\n",
    "            wandb_logger = None\n",
    "\n",
    "        f1_checkpoint_callback = ModelCheckpoint(\n",
    "            monitor=\"val_f1_score\",\n",
    "            mode=\"max\",\n",
    "            filename=\"best-val-f1-{epoch:02d}-{val_f1_score:.2f}\",\n",
    "            save_top_k=save_top_k,\n",
    "            save_on_train_epoch_end=False,\n",
    "            verbose=False,\n",
    "        )\n",
    "        early_stopping_callback = EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "            patience=early_stopping_patience,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=max_epochs,\n",
    "            num_sanity_val_steps=0,\n",
    "            log_every_n_steps=0,\n",
    "            # callbacks=[\n",
    "            #     f1_checkpoint_callback,\n",
    "            #     early_stopping_callback\n",
    "            # ],\n",
    "            default_root_dir=logs_folder,\n",
    "            logger=wandb_logger if using_wandb else None,\n",
    "        )\n",
    "\n",
    "        trainer.fit(graph_model, datamodule=data_module)\n",
    "\n",
    "        test_results = []\n",
    "        test_elapsed = []\n",
    "        results = trainer.test(graph_model, datamodule=data_module)\n",
    "        print(f\"==>> results: {results}\")\n",
    "        test_elapsed.append(results[0][\"_elapsed\"])\n",
    "\n",
    "        elapsed[model_name] = np.mean(test_elapsed).item()\n",
    "\n",
    "    time_elapsed_dict[dataset.name] = elapsed\n",
    "\n",
    "print(f\"==>> time_elapsed_dict: {time_elapsed_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the nested dictionary into a DataFrame\n",
    "df = pd.DataFrame.from_dict(time_elapsed_dict, orient='index')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average time for each model (i.e.,column-wise mean)\n",
    "average_times = df.mean(axis=0)\n",
    "average_times"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
